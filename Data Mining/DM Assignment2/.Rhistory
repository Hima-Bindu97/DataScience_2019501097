mean(sampleV2_1)
max(sampleV2_1)
var(sampleV2_1)
quantile(sampleV2_1,.25)
sampleV2_2 = stock[sample2,8]
mean(sampleV2_2)
max(sampleV2_2)
var(sampleV2_2)
quantile(sampleV2_2,.25)
#Column - 9 (V3)
sampleV3_1 = stock[sample1,9]
mean(sampleV3_1)
max(sampleV3_1)
var(sampleV3_1)
quantile(sampleV3_1,.25)
sampleV3_2 = stock[sample2,9]
mean(sampleV3_2)
max(sampleV3_2)
var(sampleV3_2)
quantile(sampleV3_2,.25)
#Column - 10 (V4)
sampleV4_1 = stock[sample1,10]
mean(sampleV4_1)
max(sampleV4_1)
var(sampleV4_1)
quantile(sampleV4_1,.25)
sampleV4_2 = stock[sample2,10]
mean(sampleV4_2)
max(sampleV4_2)
var(sampleV4_2)
quantile(sampleV4_2,.25)
#Column - 11 (V5)
sampleV5_1 = stock[sample1,11]
mean(sampleV5_1)
max(sampleV5_1)
var(sampleV5_1)
quantile(sampleV5_1,.25)
sampleV5_2 = stock[sample2,11]
mean(sampleV5_2)
max(sampleV5_2)
var(sampleV5_2)
quantile(sampleV5_2,.25)
#Column - 12 (V6)
sampleV6_1 = stock[sample1,12]
mean(sampleV6_1)
max(sampleV6_1)
var(sampleV6_1)
quantile(sampleV6_1,.25)
sampleV6_2 = stock[sample2,12]
mean(sampleV6_2)
max(sampleV6_2)
var(sampleV6_2)
quantile(sampleV6_2,.25)
#Column - 13 (V7)
sampleV7_1 = stock[sample1,13]
mean(sampleV7_1)
max(sampleV7_1)
var(sampleV7_1)
quantile(sampleV7_1,.25)
sampleV7_2 = stock[sample2,13]
mean(sampleV7_2)
max(sampleV7_2)
var(sampleV7_2)
quantile(sampleV7_2,.25)
#Compute the same quantities in part b on the entire data set and show your answers.
#How much do they differ from your answers in part b? Do you find any significant difference between two sample values
#like mean in comparison with entire data? If so what explanation you can give for that?
#Column 8
mean(stock$diffV2)
max(stock$diffV2)
var(stock$diffV2)
quantile(stock$diffV2,.25)
#how much they differ?
abs(mean(sampleV2_1)-mean(stock$diffV2))
abs(max(sampleV2_1)-max(stock$diffV2))
abs(var(sampleV2_1)-var(stock$diffV2))
abs(quantile(sampleV2_1,.25)-quantile(stock$diffV2,.25))
abs(mean(sampleV2_2)-mean(stock$diffV2))
abs(max(sampleV2_2)-max(stock$diffV2))
abs(var(sampleV2_2)-var(stock$diffV2))
abs(quantile(sampleV2_2,.25)-quantile(stock$diffV2,.25))
#The difference between these two sample valeues is : Difference is low when the sample size is large
#The maximum value of the column is nearer to the value of larger sample size
#The differnce in variance is higher when the sample size is more
#Column 9
mean(stock$diffV3)
max(stock$diffV3)
var(stock$diffV3)
quantile(stock$diffV3,.25)
#how much they differ?
abs(mean(sampleV3_1)-mean(stock$diffV3))
abs(max(sampleV3_1)-max(stock$diffV3))
abs(var(sampleV3_1)-var(stock$diffV3))
abs(quantile(sampleV3_1,.25)-quantile(stock$diffV3,.25))
abs(mean(sampleV3_2)-mean(stock$diffV3))
abs(max(sampleV3_2)-max(stock$diffV3))
abs(var(sampleV3_2)-var(stock$diffV3))
abs(quantile(sampleV3_2,.25)-quantile(stock$diffV3,.25))
#Column 10
mean(stock$diffV4)
max(stock$diffV4)
var(stock$diffV4)
quantile(stock$diffV4,.25)
#how much they differ?
abs(mean(sampleV4_1)-mean(stock$diffV4))
abs(max(sampleV4_1)-max(stock$diffV4))
abs(var(sampleV4_1)-var(stock$diffV4))
abs(quantile(sampleV4_1,.25)-quantile(stock$diffV4,.25))
abs(mean(sampleV4_2)-mean(stock$diffV4))
abs(max(sampleV4_2)-max(stock$diffV4))
abs(var(sampleV4_2)-var(stock$diffV4))
abs(quantile(sampleV4_2,.25)-quantile(stock$diffV4,.25))
#Column 11
mean(stock$diffV5)
max(stock$diffV5)
var(stock$diffV5)
quantile(stock$diffV5,.25)
#how much they differ?
abs(mean(sampleV5_1)-mean(stock$diffV5))
abs(max(sampleV5_1)-max(stock$diffV5))
abs(var(sampleV5_1)-var(stock$diffV5))
abs(quantile(sampleV5_1,.25)-quantile(stock$diffV5,.25))
abs(mean(sampleV5_2)-mean(stock$diffV5))
abs(max(sampleV5_2)-max(stock$diffV5))
abs(var(sampleV5_2)-var(stock$diffV5))
abs(quantile(sampleV5_2,.25)-quantile(stock$diffV5,.25))
#Column 12
mean(stock$diffV6)
max(stock$diffV6)
var(stock$diffV6)
quantile(stock$diffV6,.25)
#how much they differ?
abs(mean(sampleV6_1)-mean(stock$diffV6))
abs(max(sampleV6_1)-max(stock$diffV6))
abs(var(sampleV6_1)-var(stock$diffV6))
abs(quantile(sampleV6_1,.25)-quantile(stock$diffV6,.25))
abs(mean(sampleV6_2)-mean(stock$diffV6))
abs(max(sampleV6_2)-max(stock$diffV6))
abs(var(sampleV6_2)-var(stock$diffV6))
abs(quantile(sampleV6_2,.25)-quantile(stock$diffV6,.25))
#Column 13
mean(stock$diffV7)
max(stock$diffV7)
var(stock$diffV7)
quantile(stock$diffV7,.25)
#how much they differ?
abs(mean(sampleV7_1)-mean(stock$diffV7))
abs(max(sampleV7_1)-max(stock$diffV7))
abs(var(sampleV7_1)-var(stock$diffV7))
abs(quantile(sampleV7_1,.25)-quantile(stock$diffV7,.25))
abs(mean(sampleV7_2)-mean(stock$diffV7))
abs(max(sampleV7_2)-max(stock$diffV7))
abs(var(sampleV7_2)-var(stock$diffV7))
abs(quantile(sampleV7_2,.25)-quantile(stock$diffV7,.25))
#Use R to produce a single graph displaying a boxplot for open, close, high and low.
#Include the R commands and the plot.
boxplot(stock$diffV2,
stock$diffV3,
stock$diffV4,
stock$diffV5,
stock$diffV6,
stock$diffV7
,col = 'blue', main = 'Boxplot', names=c("Open","High", "Low", "Close", "volume", "adj"))
#Use R to produce a frequency histogram for Close values. Use intervals of width 2000 beginning at 0
stock$c = as.numeric(stock$V4)
hist(stock$c,breaks=seq(0,20000,by=2000),col='blue',xlab = "Close",ylab = "Frequency",main = "Histogram Plot")
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment4")
getwd()
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment4")
getwd()
test<-read.csv("sonar_test.csv", header=FALSE)
train<-read.csv("sonar_train.csv", header=FALSE)
y<-as.factor(train[,61])
x<-train[,1:60]
y_test<-as.factor(test[,61])
x_test<-test[,1:60]
library(rpart)
fit<- rpart(y~.,x,control=rpart.control(minsplit=0,minbucket=0,cp=-1, maxcompete=0, maxsurrogate=0, usesurrogate=0, xval=0,maxdepth=5))
error = 1-sum(y_test==predict(fit,x_test, type="class"))/length(y_test)
cat("Misclassification Error:",error)
library(randomForest)
fit<-randomForest(x,y)
error_rate = 1-sum(y==predict(fit,x))/length(y)
cat("Misclassification Error rate:",error_rate)
library(class)
fit_train<-knn(x,x,y,k=5)
train_error = 1-sum(y==fit_train)/length(y)
cat("Train Error rate:",train_error)
fit_test<-knn(x,x_test,y,k=5)
test_error= 1-sum(y_test==fit_test)/length(y_test)
cat("\n Test Error rate:",test_error)
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment4")
getwd()
test<-read.csv("sonar_test.csv", header=FALSE)
train<-read.csv("sonar_train.csv", header=FALSE)
y<-as.factor(train[,61])
x<-train[,1:60]
y_test<-as.factor(test[,61])
x_test<-test[,1:60]
library(rpart)
fit<- rpart(y~.,x,control=rpart.control(minsplit=0,minbucket=0,cp=-1, maxcompete=0, maxsurrogate=0, usesurrogate=0, xval=0,maxdepth=5))
error = 1-sum(y_test==predict(fit,x_test, type="class"))/length(y_test)
cat("Misclassification Error:",error)
install.packages("randomForest")
library(randomForest)
fit<-randomForest(x,y)
error_rate = 1-sum(y==predict(fit,x))/length(y)
cat("Misclassification Error rate:",error_rate)
library(class)
fit_train<-knn(x,x,y,k=5)
train_error = 1-sum(y==fit_train)/length(y)
cat("Train Error rate:",train_error)
fit_test<-knn(x,x_test,y,k=5)
test_error= 1-sum(y_test==fit_test)/length(y_test)
cat("\n Test Error rate:",test_error)
setwd("C:\Users\DELL\Desktop\DS\DataScience_2019501097\Data Mining\DM Assignment5")
data<-read.csv("sonar_test.csv", header=FALSE)
x<-data[,1:2]
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
fit<-kmeans(x, 2)
points(fit$centers,pch=19,col="blue",cex=2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
points(x,col=1+1*as.numeric(knnfit),pch=19)
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment5")
data<-read.csv("sonar_test.csv", header=FALSE)
x<-data[,1:2]
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
fit<-kmeans(x, 2)
points(fit$centers,pch=19,col="blue",cex=2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
points(x,col=1+1*as.numeric(knnfit),pch=19)
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment5")
data<-read.csv("sonar_test.csv", header=FALSE)
x<-data[,1:2]
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
fit<-kmeans(x, 2)
points(fit$centers,pch=19,col="blue",cex=2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
points(x,col=1+1*as.numeric(knnfit),pch=19)
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment5")
data<-read.csv("sonar_test.csv", header=FALSE)
x<-data[,1:2]
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
fit<-kmeans(x, 2)
points(fit$centers,pch=19,col="blue",cex=2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
points(x,col=1+1*as.numeric(knnfit),pch=19)
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
y<-data[,61]
points(x,col=2+2*y,pch=19)
1-sum(knnfit==y)/length(y)
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment5")
data<-read.csv("sonar_test.csv", header=FALSE)
x<-data[,1:2]
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
fit<-kmeans(x, 2)
points(fit$centers,pch=19,col="blue",cex=2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
points(x,col=1+1*as.numeric(knnfit),pch=19)
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
y<-data[,61]
points(x,col=2+2*y,pch=19)
1-sum(knnfit==y)/length(y)
x<-data[,1:60]
fit<-kmeans(x, 2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
1-sum(knnfit==y)/length(y)
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment5")
data<-read.csv("sonar_test.csv", header=FALSE)
x<-data[,1:2]
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
fit<-kmeans(x, 2)
points(fit$centers,pch=19,col="blue",cex=2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
points(x,col=1+1*as.numeric(knnfit),pch=19)
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
y<-data[,61]
points(x,col=2+2*y,pch=19)
1-sum(knnfit==y)/length(y)
x<-data[,1:60]
fit<-kmeans(x, 2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
1-sum(knnfit==y)/length(y)
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
center1<-1
center2<-2
for (k in 2:10){
cluster1<-x[abs(x-center1[k-1])<=abs(x-center2[k-1])]
cluster2<-x[abs(x-center1[k-1])>abs(x-center2[k-1])]
center1[k]<-mean(cluster1)
center2[k]<-mean(cluster2)
}
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
center1<-1
center2<-2
for (k in 2:10){
cluster1<-x[abs(x-center1[k-1])<=abs(x-center2[k-1])]
cluster2<-x[abs(x-center1[k-1])>abs(x-center2[k-1])]
center1[k]<-mean(cluster1)
center2[k]<-mean(cluster2)
}
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment5")
data<-read.csv("sonar_test.csv", header=FALSE)
x<-data[,1:2]
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
fit<-kmeans(x, 2)
points(fit$centers,pch=19,col="blue",cex=2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
points(x,col=1+1*as.numeric(knnfit),pch=19)
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
y<-data[,61]
points(x,col=2+2*y,pch=19)
1-sum(knnfit==y)/length(y)
x<-data[,1:60]
fit<-kmeans(x, 2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
1-sum(knnfit==y)/length(y)
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
center1<-1
center2<-2
for (k in 2:10){
cluster1<-x[abs(x-center1[k-1])<=abs(x-center2[k-1])]
cluster2<-x[abs(x-center1[k-1])>abs(x-center2[k-1])]
center1[k]<-mean(cluster1)
center2[k]<-mean(cluster2)
}
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
center1<-1
center2<-2
for (k in 2:10){
cluster1<-x[abs(x-center1[k-1])<=abs(x-center2[k-1])]
cluster2<-x[abs(x-center1[k-1])>abs(x-center2[k-1])]
center1[k]<-mean(cluster1)
center2[k]<-mean(cluster2)
}
kmeans(x,2)
plot(x, col=fit$cluster, xlab = 'x', ylab='values', pch=19)
x1<-c(1,2)
x2<-c(5,10)
data<-(rbind(x1,x2))
dist(data)
x1<-c(1,2,3,6)
x2<-c(5,10,4,12)
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
dist(data)
data<-read.csv("spring2008exams.csv")
mean_exam<-mean(data[,2],na.rm=TRUE)
sd_exam<-sd(data[,2],na.rm=TRUE)
z<-(data[,2]-mean_exam)/sd_exam
li=sort(z)
cat("largest z score:",li[length(li)])
cat("\nSmallest z score:",li[1])
spring_data<-read.csv("spring2008exams.csv")
mean_exam<-mean(spring_data[,3],na.rm=TRUE)
sd_exam<-sd(spring_data[,3],na.rm=TRUE)
z<-(spring_data[,3]-mean_exam)/sd_exam
li=sort(z)
cat("largest z score:",li[length(li)])
cat("\nSmallest z score:",li[1])
spring_data<-read.csv("spring2008exams.csv")
q1<-quantile(spring_data[,3],.25,na.rm=TRUE)
q3<-quantile(spring_data[,3],.75,na.rm=TRUE)
iqr<-q3-q1
spring_data[(spring_data[,3]>q3+1.5*iqr),3]
spring_data[(spring_data[,3]<q1-1.5*iqr),3]
boxplot(spring_data[,2],spring_data[,3],col="blue",
main="spring2008exams",
names=c("first midterm","second midterm"),ylab="Exam Score")
spring_data<-read.csv("spring2008exams.csv")
model<-lm(spring_data[,3]~spring_data[,2])
plot(spring_data[,2],spring_data[,3],pch=19,xlab="first midterm",ylab="second midterm",xlim=c(100,200),ylim=c(100,200))
abline(model)
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment5")
data<-read.csv("sonar_test.csv", header=FALSE)
x<-data[,1:2]
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
fit<-kmeans(x, 2)
points(fit$centers,pch=19,col="blue",cex=2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
points(x,col=1+1*as.numeric(knnfit),pch=19)
plot(x,pch=19,xlab=expression(x[1]), ylab=expression(x[2]))
y<-data[,61]
points(x,col=2+2*y,pch=19)
1-sum(knnfit==y)/length(y)
x<-data[,1:60]
fit<-kmeans(x, 2)
library(class)
knnfit<-knn(fit$centers,x,as.factor(c(-1,1)))
1-sum(knnfit==y)/length(y)
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
center1<-1
center2<-2
for (k in 2:10){
cluster1<-x[abs(x-center1[k-1])<=abs(x-center2[k-1])]
cluster2<-x[abs(x-center1[k-1])>abs(x-center2[k-1])]
center1[k]<-mean(cluster1)
center2[k]<-mean(cluster2)
}
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
center1<-1
center2<-2
for (k in 2:10){
cluster1<-x[abs(x-center1[k-1])<=abs(x-center2[k-1])]
cluster2<-x[abs(x-center1[k-1])>abs(x-center2[k-1])]
center1[k]<-mean(cluster1)
center2[k]<-mean(cluster2)
}
kmeans(x,2)
plot(x, col=fit$cluster, xlab = 'x', ylab='values', pch=19)
x1<-c(1,2)
x2<-c(5,10)
data<-(rbind(x1,x2))
dist(data)
x1<-c(1,2,3,6)
x2<-c(5,10,4,12)
data<-(rbind(x1,x2))
dist(data)
data<-read.csv("spring2008exams.csv")
mean_exam<-mean(data[,2],na.rm=TRUE)
sd_exam<-sd(data[,2],na.rm=TRUE)
z<-(data[,2]-mean_exam)/sd_exam
li=sort(z)
cat("largest z score:",li[length(li)])
cat("\nSmallest z score:",li[1])
spring_data<-read.csv("spring2008exams.csv")
mean_exam<-mean(spring_data[,3],na.rm=TRUE)
sd_exam<-sd(spring_data[,3],na.rm=TRUE)
z<-(spring_data[,3]-mean_exam)/sd_exam
li=sort(z)
cat("largest z score:",li[length(li)])
cat("\nSmallest z score:",li[1])
spring_data<-read.csv("spring2008exams.csv")
q1<-quantile(spring_data[,3],.25,na.rm=TRUE)
q3<-quantile(spring_data[,3],.75,na.rm=TRUE)
iqr<-q3-q1
spring_data[(spring_data[,3]>q3+1.5*iqr),3]
spring_data[(spring_data[,3]<q1-1.5*iqr),3]
boxplot(spring_data[,2],spring_data[,3],col="blue",
main="spring2008exams",
names=c("first midterm","second midterm"),ylab="Exam Score")
spring_data<-read.csv("spring2008exams.csv")
model<-lm(spring_data[,3]~spring_data[,2])
plot(spring_data[,2],spring_data[,3],pch=19,xlab="first midterm",ylab="second midterm",xlim=c(100,200),ylim=c(100,200))
abline(model)
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment2")
getwd()
myData<-read.csv("myfirstdata.csv",header=FALSE)
str(myData)
plot(myData[,1])
setwd("/Users/pemawangmo/Desktop/DS_Notes/DATA_MINING/DataMiningAssignments/DM_Assignment2")
setwd("/Users/pemawangmo/Desktop/DS_Notes/DATA_MINING/DataMiningAssignments/DM_Assignment2")
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment2")
getwd()
")
myData<-read.csv("twomillion.csv",header=FALSE)
setwd("C:\\Users\\DELL\\Desktop\\DS\\DataScience_2019501097\\Data Mining\\DM Assignment2")
getwd()
myData<-read.csv("twomillion.csv",header=FALSE)
data<-sample(myData[,1],10000,replace=TRUE)
mean(sample_data)
data<-sample(myData[,1],10000,replace=TRUE)
mean(data)
max(data)
var(data)
quantile(data, .25)
quantile(data, .50)
quantile(data, .75)
summary(data)
quantile(myData$V1)
mean(myData$V1)
max(myData$V1)
var(myData$V1)
median(myData$V1)
write.csv(sample_data, file = "sample_data.csv")
cahouse<-read.csv("CA_house_prices.csv", header=FALSE)
ohhouse<-read.csv("OH_house_prices.csv", header = FALSE)
boxplot(cahouse$V1, ohhouse$V1, main="Box Plots",col="blue",names=c("CA", "OH"),ylab="Prices")
hist(cahouse$V1,breaks=seq(from=0,to=3500,by=500),col=c("green","red","blue","yellow","orange"),main="My Histogram",xlab="California House Prices(dollars)",ylab = "frequency")
legend(2100,.6,c("CA Houses","OH Houses"), col=c("black","red"),lwd=c(1,4))
plot(ecdf(cahouse[,1]),verticals= TRUE,do.p = FALSE,main ="ECDF for House Prices ",xlab="Prices (in thousands)",ylab="Frequency")
lines(ecdf(ohio[,1]),verticals= TRUE,do.p = FALSE,col.h="red",col.v="red",lwd=4)
plot(ecdf(cahouse[,1]),verticals= TRUE,do.p = FALSE,main ="ECDF for House Prices ",xlab="Prices (in thousands)",ylab="Frequency")
lines(ecdf(ohhouse[,1]),verticals= TRUE,do.p = FALSE,col.h="red",col.v="red",lwd=4)
legend(2100,.6,c("CA Houses","OH Houses"), col=c("black","red"),lwd=c(1,4))
football<-read.csv("football.csv",header=TRUE)
head(football)
str(football)
plot(football[,2],football[,3],xlim=c(0,12),ylim=c(0,12),pch=15,col="blue",xlab="2003 Wins",ylab="2004 Wins",main="Football Wins")
cor(football[,2],football[,3])
cor(football[,2],football[,3]+10)
cor(football[,2],football[,3]*2)
cor(football[,2],football[,3]*-2)
ohhouse<-read.csv("OH_house_prices.csv", header=FALSE)
median(ohhouse[,1])
mean(ohhouse[,1])
median(ohhouse[,1]+10)
median(2*ohhouse[,1])
ages<-c(19,23,30,30,45,25,24,20)
sd(ages)
sd(100*ages)
sd(ages+10)
